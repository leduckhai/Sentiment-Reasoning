{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\n",
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "metrics = load_metric('accuracy')\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def inference(path):\n",
    "  prefix = 'summarize: ' if 'mt5' in path else ''\n",
    "  tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "  model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "  max_length = 1024 if 'bert' not in path else 256\n",
    "  def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=True)\n",
    "    labels = tokenizer(text_target=examples[\"label\"], max_length=5, truncation=True, padding=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "  testset =  pd.read_excel('test.xlsx')\n",
    "  testset['label'] = testset['label'].astype(str)\n",
    "  dataset = Dataset.from_pandas(testset[['text', 'label']])\n",
    "\n",
    "#   dataset = load_dataset(\"json\", data_files=\"datasets/faq/test/faq_test.json\", split='train')\n",
    "  test_tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "  data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "  model.to('cuda')\n",
    "\n",
    "\n",
    "  max_target_length = 5\n",
    "  test_tokenized_datasets = test_tokenized_datasets.remove_columns(['text', 'label'])\n",
    "  dataloader = torch.utils.data.DataLoader(test_tokenized_datasets, collate_fn=data_collator, batch_size=32)\n",
    "\n",
    "  predictions = []\n",
    "  references = []\n",
    "  for i, batch in enumerate(tqdm(dataloader)):\n",
    "  outputs = model.generate(\n",
    "    input_ids=batch['input_ids'].to('cuda'),\n",
    "    max_length=max_target_length,\n",
    "    attention_mask=batch['attention_mask'].to('cuda'),\n",
    "  )\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    outputs = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in outputs]\n",
    "\n",
    "    labels = np.where(batch['labels'] != -100,  batch['labels'], tokenizer.pad_token_id)\n",
    "    actuals = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in labels]\n",
    "  predictions.extend(outputs)\n",
    "  references.extend(actuals)\n",
    "  metrics.add_batch(predictions=outputs, references=actuals)\n",
    "\n",
    "  metrics.compute()\n",
    "\n",
    "  rouges = [{k: v.mid.fmeasure} for k,v in metrics.compute(predictions=predictions, references=references).items()]\n",
    "#   new_file_path = './r_scores_faq'\n",
    "#   # Write to the file\n",
    "#   try:\n",
    "#   # Attempt to append to the file\n",
    "#   with open(new_file_path, 'a') as file:\n",
    "#     file.write(path.split('/')[-2] + '\\n')\n",
    "#     for new_content_str in rouges:\n",
    "#       result = next(iter(new_content_str))\n",
    "#       file.write(f\"{result}: {new_content_str[result]}\\n\")\n",
    "#     file.write('\\n')\n",
    "#   action_result = \"Content appended to the existing file.\"\n",
    "#   except FileNotFoundError:\n",
    "#   # File doesn't exist, create it and write the content\n",
    "#   with open(new_file_path, 'w') as file:\n",
    "#     file.write(path)\n",
    "#     file.write(new_content_str)\n",
    "#   action_result = \"File did not exist, so it was created with the new content.\"\n",
    "  \n",
    "#   del model\n",
    "#   gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from datasets import load_metric, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\n",
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "metrics = load_metric('accuracy')\n",
    "import gc\n",
    "import os\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "path = './multitask/distilling-step-by-step/ckpts/VietAI/vit5-base_human_justification/'\n",
    "# path = 'multitask/distilling-step-by-step/ckpts//'\n",
    "# path = 'results/flan-t5-base/'\n",
    "prefix = 'gt: ' if 'distilling' in path else ''\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "max_length = 1024 if 'bert' not in path else 256\n",
    "def preprocess_function(examples):\n",
    "  inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "  model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=True)\n",
    "  labels = tokenizer(text_target=examples[\"label\"], max_length=5, truncation=True, padding=True)\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "testset =  pd.read_excel('test.xlsx')\n",
    "test_with_asr = pd.read_excel('test_asr.xlsx')\n",
    "testset['text'] = test_with_asr['asr']\n",
    "testset['label'] = testset['label'].astype(str)\n",
    "dataset = Dataset.from_pandas(testset[['text', 'label']])\n",
    "\n",
    "#   dataset = load_dataset(\"json\", data_files=\"datasets/faq/test/faq_test.json\", split='train')\n",
    "test_tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "model.to('cuda:2')\n",
    "\n",
    "\n",
    "max_target_length = 25\n",
    "test_tokenized_datasets = test_tokenized_datasets.remove_columns(['text', 'label'])\n",
    "dataloader = torch.utils.data.DataLoader(test_tokenized_datasets.select(idx), collate_fn=data_collator, batch_size=32)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "for i, batch in enumerate(tqdm(dataloader)):\n",
    "  outputs = model.generate(\n",
    "  input_ids=batch['input_ids'].to('cuda:2'),\n",
    "  max_length=max_target_length,\n",
    "  attention_mask=batch['attention_mask'].to('cuda:2'),\n",
    "  )\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "      outputs = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in outputs]\n",
    "\n",
    "      labels = np.where(batch['labels'] != -100,  batch['labels'], tokenizer.pad_token_id)\n",
    "      actuals = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in labels]\n",
    "      predictions.extend(outputs)\n",
    "      references.extend(actuals)\n",
    "# metrics.add_batch(predictions=outputs, references=actuals)\n",
    "\n",
    "# metrics.compute()\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "  decoded_preds, decoded_labels = predictions, references\n",
    "#   logits = np.argmax(logits, axis=1)\n",
    "  decoded_preds = [pred if pred.isdigit() else '-1' for pred in decoded_preds]  # Replace non-digit predictions with '-1'\n",
    "  decoded_labels = [label if label.isdigit() else '-1' for label in decoded_labels]  # Replace non-digit labels with '-1'\n",
    "  predictions = decoded_preds\n",
    "  labels = decoded_labels\n",
    "  neg,neu,pos = f1.compute(predictions=predictions, references=labels, average=None)['f1']\n",
    "  metrics_result = {\n",
    "    \"accuracy\": accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "    \"macro_f1\": f1.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "#   \"macro_precision\": precision.compute(predictions=predictions, references=labels, average='macro')['precision'],\n",
    "#   \"macro_recall\": recall.compute(predictions=predictions, references=labels, average='macro')['recall'],\n",
    "    \"f1_neg\": neg,\n",
    "    \"f1_neu\": neu,\n",
    "    \"f1_pos\": pos\n",
    "\n",
    "  }\n",
    "  return metrics_result\n",
    "# rouges = [{k: v.mid.fmeasure} for k,v in metrics.compute(predictions=predictions, references=references).items()]\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "print(compute_metrics(predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "from datasets import Dataset, load_metric\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import argparse\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "base_model_name = 'vtrungnhan9/vmlu-llm'\n",
    "print(\"loading\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir='./models')\n",
    "model = PeftModel.from_pretrained(model, './results/vmlu-llm_human_justificationv2/')\n",
    "print('finished loadding')\n",
    "model = model.merge_and_unload()\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "train_df = pd.read_excel('train.xlsx')#pd.concat([df, df_dev]).reset_index(drop=True)\n",
    "train_df['label'] = train_df['label'].astype(str)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "testset =  pd.read_excel('test.xlsx')\n",
    "\n",
    "testset['label'] = testset['label'].astype(str)\n",
    "print(train_df['label'].unique())\n",
    "print(testset['label'].unique())\n",
    "test_dataset = Dataset.from_pandas(testset[['text', 'label']])\n",
    "\n",
    "def template(inp, out):\n",
    "    conversation = [{\"role\": \"system\", \"content\": \"Bạn là trợ lý nhận diện cảm xúc. Với một văn bản, trả lời 0 nếu cảm xúc tiêu cực, 1 nếu không có cảm xúc, 2 nếu cảm xúc tích cực.\" },\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"nhận diện cảm xúc: '{inp.strip()}'\"\"\"},\n",
    "                    {'role': 'asssistant', 'content': str(out)}\n",
    "                   ]\n",
    "#     print(out)\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "#     prompt = prompt + ' '\n",
    "    return prompt\n",
    "\n",
    "# reformatted_output = [reformat(inp, out) for inp, out in zip(dataset['train']['words'], dataset['train']['tags'])]\n",
    "new_column_train = [template(inp, out) for inp, out in zip(train_dataset['text'], train_dataset['label'])]\n",
    "train_dataset= train_dataset.add_column(\"train_text\", new_column_train)\n",
    "new_column_train = [template(inp, out) for inp, out in zip(test_dataset['text'], test_dataset['label'])]\n",
    "test_dataset= test_dataset.add_column(\"train_text\", new_column_train)\n",
    "\n",
    "outs = []\n",
    "i = 0\n",
    "# print(\"Start inference\")\n",
    "# for tt in (test_dataset['train_text']):\n",
    "#     if i % 100 == 0:\n",
    "#         print(i)\n",
    "#     input_ids = tokenizer(tt, return_tensors='pt').input_ids.cuda()\n",
    "#     out_ids = model.generate(input_ids, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id, output_scores=True)\n",
    "\n",
    "#     assistant = tokenizer.batch_decode(out_ids[:, input_ids.size(1): ], skip_special_tokens=True)[0].strip()\n",
    "# #     print(assistant)\n",
    "#     outs.append(assistant)\n",
    "#     i += 1\n",
    "# #     break\n",
    "# del model\n",
    "# gc.collect()\n",
    "outs = []\n",
    "batch_size=32\n",
    "print(\"Start inference\")\n",
    "for i in tqdm(range(0, len(test_dataset), batch_size)):\n",
    "    batch = test_dataset[i:i + batch_size]\n",
    "    inputs = tokenizer(batch['train_text'], return_tensors='pt', padding=True, truncation=True).input_ids.cuda()\n",
    "    outputs = model.generate(inputs, max_new_tokens=2, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs[:, inputs.size(1):], skip_special_tokens=True)\n",
    "    outs.extend([output.strip() for output in decoded_outputs])\n",
    "#     break\n",
    "\n",
    "# Cleanup\n",
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from datasets import load_metric, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\n",
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "metrics = load_metric('accuracy')\n",
    "import gc\n",
    "import os\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "  decoded_preds, decoded_labels = predictions, references\n",
    "#   logits = np.argmax(logits, axis=1)\n",
    "  decoded_preds = [pred if pred.isdigit() else '-1' for pred in decoded_preds]  # Replace non-digit predictions with '-1'\n",
    "  decoded_labels = [label if label.isdigit() else '-1' for label in decoded_labels]  # Replace non-digit labels with '-1'\n",
    "  predictions = decoded_preds\n",
    "  labels = decoded_labels\n",
    "  neg,neu,pos = f1.compute(predictions=predictions, references=labels, average=None)['f1']\n",
    "  metrics_result = {\n",
    "    \"accuracy\": accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "    \"macro_f1\": f1.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "#   \"macro_precision\": precision.compute(predictions=predictions, references=labels, average='macro')['precision'],\n",
    "#   \"macro_recall\": recall.compute(predictions=predictions, references=labels, average='macro')['recall'],\n",
    "    \"f1_neg\": neg,\n",
    "    \"f1_neu\": neu,\n",
    "    \"f1_pos\": pos\n",
    "\n",
    "  }\n",
    "  return metrics_result\n",
    "\n",
    "references = (testset['label'])\n",
    "compute_metrics(outs, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "from datasets import Dataset, load_metric\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import argparse\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "base_model_name = 'vtrungnhan9/vmlu-llm'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir='./models')\n",
    "# model = PeftModel.from_pretrained(model, './Vistral-7B-Chat_no')\n",
    "\n",
    "# model = model.merge_and_unload()\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "train_df = pd.read_excel('train.xlsx')#pd.concat([df, df_dev]).reset_index(drop=True)\n",
    "test_with_asr = pd.read_excel('test_asr.xlsx')\n",
    "testset['text'] = test_with_asr['asr']\n",
    "\n",
    "train_df['label'] = train_df['label'].astype(str)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "testset =  pd.read_excel('test.xlsx')\n",
    "\n",
    "testset['label'] = testset['label'].astype(str)\n",
    "print(train_df['label'].unique())\n",
    "print(testset['label'].unique())\n",
    "test_dataset = Dataset.from_pandas(testset[['text', 'label']])\n",
    "\n",
    "def template(inp, out):\n",
    "    conversation = [{\"role\": \"system\", \"content\": \"Bạn là trợ lý nhận diện cảm xúc. Với một văn bản, trả lời 0 nếu cảm xúc tiêu cực, 1 nếu không có cảm xúc, 2 nếu cảm xúc tích cực.\" },\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Bạn là trợ lý nhận diện cảm xúc. Với một văn bản, trả lời 0 nếu cảm xúc tiêu cực, 1 nếu không có cảm xúc, 2 nếu cảm xúc tích cực. Nhận diện cảm xúc: '{inp.strip()}'\"\"\"},\n",
    "#                     {'role': 'asssistant', 'content': str(out) + 'herqwwewf'}\n",
    "                   ]\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    print(prompt)\n",
    "#     prompt = prompt + f' {out}'\n",
    "    return prompt\n",
    "\n",
    "# reformatted_output = [reformat(inp, out) for inp, out in zip(dataset['train']['words'], dataset['train']['tags'])]\n",
    "new_column_train = [template(inp, out) for inp, out in zip(train_dataset['text'], train_dataset['label'])]\n",
    "train_dataset= train_dataset.add_column(\"train_text\", new_column_train)\n",
    "new_column_train = [template(inp, out) for inp, out in zip(test_dataset['text'], test_dataset['label'])]\n",
    "test_dataset= test_dataset.add_column(\"train_text\", new_column_train)\n",
    "\n",
    "outs = []\n",
    "i = 0\n",
    "# for tt in (test_dataset['train_text']):\n",
    "#     if i % 500 == 0:\n",
    "#         print(i)\n",
    "#         print(outs)\n",
    "#     input_ids = tokenizer(tt, return_tensors='pt').input_ids.cuda()\n",
    "#     out_ids = model.generate(input_ids, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#     assistant = tokenizer.batch_decode(out_ids[:, input_ids.size(1): ], skip_special_tokens=True)[0].strip()\n",
    "#     outs.append(assistant)\n",
    "# #     print(outs)\n",
    "    \n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for tt in (test_dataset['train_text']):\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "        print(outs)\n",
    "    input_ids = tokenizer(tt, return_tensors='pt').input_ids.cuda()\n",
    "    out_ids = model.generate(input_ids, max_new_tokens=2, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    assistant = tokenizer.batch_decode(out_ids[:, input_ids.size(1): ], skip_special_tokens=True)[0].strip()\n",
    "    outs.append(assistant)\n",
    "#     print(outs)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(inp, out):\n",
    "    conversation = [{\"role\": \"system\", \"content\": \"Bạn là trợ lý nhận diện cảm xúc. Với một văn bản, trả lời 0 nếu cảm xúc tiêu cực, 1 nếu không có cảm xúc, 2 nếu cảm xúc tích cực.\" },\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"Nhận diện cảm xúc: '{inp.strip()}'\"\"\"},\n",
    "#                     {'role': 'asssistant', 'content': str(out) + 'herqwwewf'}\n",
    "                   ]\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    print(prompt)\n",
    "#     prompt = prompt + f' {out}'\n",
    "    return prompt\n",
    "\n",
    "# reformatted_output = [reformat(inp, out) for inp, out in zip(dataset['train']['words'], dataset['train']['tags'])]\n",
    "new_column_train = [template(inp, out) for inp, out in zip(train_dataset['text'], train_dataset['label'])]\n",
    "train_dataset= train_dataset.add_column(\"train_text\", new_column_train)\n",
    "new_column_train = [template(inp, out) for inp, out in zip(test_dataset['text'], test_dataset['label'])]\n",
    "test_dataset= test_dataset.add_column(\"train_text\", new_column_train)\n",
    "\n",
    "outs = []\n",
    "i = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "from datasets import Dataset, load_metric\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import argparse\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "base_model_name = 'Viet-Mistral/Vistral-7B-Chat'\n",
    "print(\"loading\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir='./models')\n",
    "model = PeftModel.from_pretrained(model, './results/Vistral-7B-Chat_human_justification/')\n",
    "print('finished laoding')\n",
    "model = model.merge_and_unload()\n",
    "model = model.to('cuda:7')\n",
    "\n",
    "\n",
    "train_df = pd.read_excel('train.xlsx')#pd.concat([df, df_dev]).reset_index(drop=True)\n",
    "train_df['label'] = train_df['label'].astype(str)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "testset =  pd.read_excel('test.xlsx')\n",
    "test_with_asr = pd.read_excel('test_asr.xlsx')\n",
    "testset['text'] = test_with_asr['asr']\n",
    "\n",
    "testset['label'] = testset['label'].astype(str)\n",
    "print(train_df['label'].unique())\n",
    "print(testset['label'].unique())\n",
    "test_dataset = Dataset.from_pandas(testset[['text', 'label', 'human_justification']])\n",
    "\n",
    "def template(inp, out):\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"Bạn là trợ lý nhận diện cảm xúc. Với một văn bản, trả lời 0 nếu cảm xúc tiêu cực, 1 nếu không có cảm xúc, 2 nếu cảm xúc tích cực.\" },\n",
    "#                     {\"role\": \"user\", \"content\": f\"\"\"sentiment analysis: '{inp.strip()}'\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"nhận diện cảm xúc: '{inp.strip()}'\"\"\"},\n",
    "        #                     {'role': 'asssistant', 'content': str(out) + 'herqwwewf'}\n",
    "                   ]\n",
    "#     print(out)\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    prompt = prompt + f' '\n",
    "    return prompt\n",
    "\n",
    "# reformatted_output = [reformat(inp, out) for inp, out in zip(dataset['train']['words'], dataset['train']['tags'])]\n",
    "new_column_train = [template(inp, out) for inp, out in zip(train_dataset['text'], train_dataset['label'])]\n",
    "train_dataset= train_dataset.add_column(\"train_text\", new_column_train)\n",
    "new_column_train = [template(inp, out) for inp, out in zip(test_dataset['text'], test_dataset['label'])]\n",
    "test_dataset= test_dataset.add_column(\"train_text\", new_column_train)\n",
    "\n",
    "outs = []\n",
    "i = 0\n",
    "print(\"Start inference\")\n",
    "for tt in (test_dataset.select(idx)['train_text']):\n",
    "    if i % 100 == 0:\n",
    "        print(i, set(outs))\n",
    "    input_ids = tokenizer(tt, return_tensors='pt').input_ids.to('cuda:7')#[:,:-1]\n",
    "    out_ids = model.generate(input_ids, max_new_tokens=25, pad_token_id=tokenizer.eos_token_id, output_scores=True)\n",
    "\n",
    "    assistant = tokenizer.batch_decode(out_ids[:, input_ids.size(1): ], skip_special_tokens=True)[0].strip()\n",
    "#     print(assistant)\n",
    "    outs.append(assistant)\n",
    "    i += 1\n",
    "#     break\n",
    "#     print(assistant)\n",
    "del model\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions = [o[2:] for o in outs]\n",
    "references = test_dataset.select(idx)['human_justification']\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"vi\")\n",
    "sum(results['f1'])/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "rouge = load(\"rouge\")\n",
    "predictions = [o[2:] for o in outs]\n",
    "references = test_dataset.select(idx)['human_justification']\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = \"\"\"trả lại cho họ chất lượng cuộc sống bình thường như bao người khác là được nghe được nói thế nhưng điều kỳ diệu đã xảy\n",
    "những chia sẻ vô cùng hữu ích và thiết thực vừa rồi ạ có thể thấy là hầu hết người bệnh nằm điều trị trong\n",
    "khám suốt tiểu đường nó vẫn mệt mỏi vô khám tai biến bộ não vô khám nhưng mà xương thì nó loãng xương rất là nhiều\n",
    "\"\"\".split('\\n')\n",
    "\n",
    "testdf = test_dataset.to_pandas()\n",
    "testdf[testdf.text.isin(test_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = []\n",
    "outs = []\n",
    "i = 0\n",
    "\"\"\"trả lại cho họ chất lượng cuộc sống bình thường như bao người khác là được nghe được nói thế nhưng điều kỳ diệu đã xảy\n",
    "những chia sẻ vô cùng hữu ích và thiết thực vừa rồi ạ có thể thấy là hầu hết người bệnh nằm điều trị trong\n",
    "khám suốt tiểu đường nó vẫn mệt mỏi vô khám tai biến bộ não vô khám nhưng mà xương thì nó loãng xương rất là nhiều\n",
    "\"\"\".split()\n",
    "for tt in (testdf[testdf.text.isin(test_samples)]['train_text']):\n",
    "    if i % 100 == 0:\n",
    "        print(i, set(outs))\n",
    "    input_ids = tokenizer(tt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(input_ids, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id, output_scores=True, output_attentions=True, return_dict_in_generate=True)\n",
    "\n",
    "    assistant = (output.scores)[0].softmax(dim=1)[:,output.sequences[:, input_ids.size(1) ].item()].item()\n",
    "#     print(assistant)\n",
    "    confidence.append(assistant)\n",
    "    \n",
    "    assistant = tokenizer.batch_decode(output.sequences[:, input_ids.size(1): ], skip_special_tokens=True)[0].strip()\n",
    "    outs.append(assistant)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from datasets import load_metric, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments\n",
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "metrics = load_metric('accuracy')\n",
    "import gc\n",
    "import os\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "  decoded_preds, decoded_labels = predictions, references\n",
    "#   logits = np.argmax(logits, axis=1)\n",
    "  decoded_preds = [pred if pred.isdigit() else '-1' for pred in decoded_preds]  # Replace non-digit predictions with '-1'\n",
    "  decoded_labels = [label if label.isdigit() else '-1' for label in decoded_labels]  # Replace non-digit labels with '-1'\n",
    "  predictions = decoded_preds\n",
    "  labels = decoded_labels\n",
    "  neg,neu,pos = f1.compute(predictions=predictions, references=labels, average=None)['f1']\n",
    "  metrics_result = {\n",
    "    \"accuracy\": accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "    \"macro_f1\": f1.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "#   \"macro_precision\": precision.compute(predictions=predictions, references=labels, average='macro')['precision'],\n",
    "#   \"macro_recall\": recall.compute(predictions=predictions, references=labels, average='macro')['recall'],\n",
    "    \"f1_neg\": neg,\n",
    "    \"f1_neu\": neu,\n",
    "    \"f1_pos\": pos\n",
    "\n",
    "  }\n",
    "  return metrics_result\n",
    "\n",
    "references = (testset['label'])\n",
    "compute_metrics(outs, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "from datasets import Dataset, load_metric\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "import argparse\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "base_model_name = 'Viet-Mistral/Vistral-7B-Chat'\n",
    "print(\"loading\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, cache_dir='./models')\n",
    "model = PeftModel.from_pretrained(model, './results/Vistral-7B-Chat_human_justification/')\n",
    "print('finished laoding')\n",
    "model = model.merge_and_unload()\n",
    "model = model.to('cuda')\n",
    "\n",
    "\n",
    "train_df = pd.read_excel('train.xlsx')#pd.concat([df, df_dev]).reset_index(drop=True)\n",
    "train_df['label'] = train_df['label'].astype(str)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "testset =  pd.read_excel('test.xlsx')\n",
    "# test_with_asr = pd.read_excel('test_asr.xlsx')\n",
    "# testset['text'] = test_with_asr['asr']\n",
    "\n",
    "testset['label'] = testset['label'].astype(str)\n",
    "print(train_df['label'].unique())\n",
    "print(testset['label'].unique())\n",
    "test_dataset = Dataset.from_pandas(testset[['text', 'label']])\n",
    "\n",
    "def template(inp, out):\n",
    "    conversation = [{\"role\": \"system\", \"content\": \"Bạn là trợ lý nhận diện cảm xúc. Với một văn bản, trả lời 0 nếu cảm xúc tiêu cực, 1 nếu không có cảm xúc, 2 nếu cảm xúc tích cực.\" },\n",
    "                    {\"role\": \"user\", \"content\": f\"\"\"nhận diện cảm xúc: '{inp.strip()}'\"\"\"},\n",
    "#                     {'role': 'asssistant', 'content': str(out) + 'herqwwewf'}\n",
    "                   ]\n",
    "#     print(out)\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "    prompt = prompt + f' '\n",
    "    return prompt\n",
    "\n",
    "# reformatted_output = [reformat(inp, out) for inp, out in zip(dataset['train']['words'], dataset['train']['tags'])]\n",
    "new_column_train = [template(inp, out) for inp, out in zip(train_dataset['text'], train_dataset['label'])]\n",
    "train_dataset= train_dataset.add_column(\"train_text\", new_column_train)\n",
    "new_column_train = [template(inp, out) for inp, out in zip(test_dataset['text'], test_dataset['label'])]\n",
    "test_dataset= test_dataset.add_column(\"train_text\", new_column_train)\n",
    "\n",
    "outs = []\n",
    "i = 0\n",
    "print(\"Start inference\")\n",
    "\n",
    "confidence = []\n",
    "i = 0\n",
    "for tt in (test_dataset['train_text']):\n",
    "    if i % 100 == 0:\n",
    "        print(i, set(outs))\n",
    "    input_ids = tokenizer(tt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(input_ids, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id, output_scores=True, output_attentions=True, return_dict_in_generate=True)\n",
    "\n",
    "    assistant = (output.scores)[0].softmax(dim=1)[:,output.sequences[:, input_ids.size(1) ].item()].item()\n",
    "#     print(assistant)\n",
    "    confidence.append(assistant)\n",
    "    \n",
    "    assistant = tokenizer.batch_decode(output.sequences[:, input_ids.size(1): ], skip_special_tokens=True)[0].strip()\n",
    "    outs.append(assistant)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"Start inference\")\n",
    "\n",
    "confidence = []\n",
    "i = 0\n",
    "for tt in (test_dataset['train_text']):\n",
    "    if i % 100 == 0:\n",
    "        print(i, set(outs))\n",
    "    input_ids = tokenizer(tt, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(input_ids, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id, output_scores=True, output_attentions=True, return_dict_in_generate=True)\n",
    "\n",
    "    assistant = (output.scores)[0].softmax(dim=1)[:,output.sequences[:, input_ids.size(1) ].item()].item()\n",
    "#     print(assistant)\n",
    "    confidence.append(assistant)\n",
    "    \n",
    "    assistant = tokenizer.batch_decode(output.sequences[:, input_ids.size(1): ], skip_special_tokens=True)[0].strip()\n",
    "    outs.append(assistant)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distill",
   "language": "python",
   "name": "distill"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
